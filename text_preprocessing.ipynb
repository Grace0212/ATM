{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Grace0212/ATM/blob/main/text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwrJCtF7ZGpx"
      },
      "source": [
        "<h1 style=\"text-align: center;\">Text Preprocessing 1 - Tutorial</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYgrxUWOZGpy"
      },
      "source": [
        "Text Pre-processing common steps:\n",
        "\n",
        "1. Text Cleaning: special characters, HTML tags, new lines\n",
        "2. Tokenization: split text into sentences and words.\n",
        "3. Stop Words Removal: remove words of little value like \"the\", \"and\", \"a\", \"an\".\n",
        "4. Stemming: stripping the affixes from words.\n",
        "5. Lemmatization: converting words to their base form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "HamxbBciZGpz"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "h5EQQCMDZGpz"
      },
      "outputs": [],
      "source": [
        "!pip3 install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9mDMYucBZGp0"
      },
      "outputs": [],
      "source": [
        "example_sentence = \"\"\"A fair number of brave souls who upgraded their SI clock oscillator have\n",
        "shared their experiences for this poll. Please send a brief message detailing\n",
        "your experiences with the procedure. <br> Top speed attained, CPU rated speed,\n",
        "add on cards & adapters, heat sinks, hour of usage per day, floppy disk\n",
        "functionality with 800 and 1.4m floppies are especially requested.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fjflu9YZGp1"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FYO3wpsOZGp1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)  # Remove HTML tags\n",
        "    text = re.sub(r\"\\\\n\", \" \", text)  # Remove explicit new-line characters\n",
        "    text = re.sub(r\"[^\\w\\s.]\", \" \", text)  # Remove special characters except for decimal points\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
        "    return text.strip().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "zPQmQ8RWZGp1"
      },
      "outputs": [],
      "source": [
        "cleaned_text = clean_text(example_sentence)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk2_OEoeZGp2"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "c2EukQKIZGp2"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab') #model for sentence tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "K3QGQyt9ZGp2"
      },
      "outputs": [],
      "source": [
        "#Sentence Tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "tokenized_sent = sent_tokenize(cleaned_text)\n",
        "print('number of sentences: ', len(tokenized_sent))\n",
        "print(tokenized_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "01xW1bZpZGp3"
      },
      "outputs": [],
      "source": [
        "#Word Tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokenized = word_tokenize(cleaned_text)\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "b-TVKqHTZGp3"
      },
      "outputs": [],
      "source": [
        "#Tweet Tokenizer compared to word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet = \"Dont take cryptocurrency advice from people on Twitter üòÉüëç #crypto\"\n",
        "tokenizer = TweetTokenizer()\n",
        "tokenized_tweet = tokenizer.tokenize(tweet)\n",
        "print(tokenized_tweet)\n",
        "print(word_tokenize(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI3ey-38ZGp3"
      },
      "source": [
        "## Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr0xFygsZGp4"
      },
      "source": [
        "### 1- NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "pJhImeYXZGp4"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS-jc54pZGp4"
      },
      "source": [
        "Remember That:\n",
        "* Porter Stemmer removes suffixes in a rule-based manner\n",
        "* It does not always return valid English words\n",
        "* Some words retain meaningful roots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "fBtLMmt7ZGp4"
      },
      "outputs": [],
      "source": [
        "# Standard cases\n",
        "print(porter.stem('argue'))\n",
        "print(porter.stem('argued'))\n",
        "print(porter.stem('argues'))\n",
        "print(porter.stem('arguing'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZvlEYLQpZGp4"
      },
      "outputs": [],
      "source": [
        "# Plurals and derivational forms\n",
        "print(porter.stem('running'))\n",
        "print(porter.stem('runner'))\n",
        "print(porter.stem('flies'))\n",
        "print(porter.stem('fly'))\n",
        "print(porter.stem('crying'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "x7y3rFVUZGp5"
      },
      "outputs": [],
      "source": [
        "# Complex endings\n",
        "print(porter.stem('happiness'))\n",
        "print(porter.stem('university'))\n",
        "print(porter.stem('national'))\n",
        "print(porter.stem('generalization'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2RB5ir3ZGp5"
      },
      "source": [
        "## When to Use PorterStemmer?\n",
        "* For lightweight and rule-based stemming such as text classification and IR systems.\n",
        "* For more linguistically accurate results, consider lemmatization instead (e.g., WordNetLemmatizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "oxFBt0L4ZGp5"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "pmGUKJmbZGp5"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"argue\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"argued\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"argues\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"arguing\", 'n'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "J8tePEp9ZGp5"
      },
      "outputs": [],
      "source": [
        "print(lemmatizer.lemmatize(\"better\", 'a'))\n",
        "print(lemmatizer.lemmatize(\"running\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"running\", 'n'))\n",
        "print(lemmatizer.lemmatize(\"flies\", 'n'))\n",
        "print(lemmatizer.lemmatize(\"flies\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"mice\", 'n'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "wrtGeJj6ZGp6"
      },
      "outputs": [],
      "source": [
        "#WordNetLemmatizer requires the correct POS tag to be accurate, default is noun\n",
        "print(lemmatizer.lemmatize(\"went\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM_aemGFZGp6"
      },
      "source": [
        "### 2- spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lQzDptCCZGp6"
      },
      "outputs": [],
      "source": [
        "!pip3 install spacy\n",
        "!python3 -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "pNIsqj5bZGp6"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md') #load the core English language model"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "dBxiUZWzZGp6"
      },
      "source": [
        "SpaCy has Built-in POS tagging (no need to separately tag words like in NLTK)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TfjrQw1sZGp6"
      },
      "outputs": [],
      "source": [
        "doc=nlp('After the cats fell asleep, the mice went out to play.')\n",
        "for token in doc:\n",
        "    print(token,'-->',token.lemma_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0AcuOWJiZGp7"
      },
      "outputs": [],
      "source": [
        "#lemmatize our original example sentence\n",
        "doc = nlp(cleaned_text)\n",
        "\n",
        "# Extract original words and their lemmatized forms\n",
        "original = [token.text for token in doc]\n",
        "lemmatized = [token.lemma_.lower() for token in doc]\n",
        "\n",
        "# Display results in aligned format\n",
        "print(f\"{'Original':<15} {'Lemmatized':<15}\")\n",
        "print(\"=\" * 30)\n",
        "for orig, lem in zip(original, lemmatized):\n",
        "    print(f\"{orig:<15} {lem:<15}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNZP_azeZGp7"
      },
      "source": [
        "## Stop Word Removal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "qtUsbsJZZGp7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HKJoAaNBZGp7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get stop words list\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# Convert to a set to remove duplicates\n",
        "unique_stopwords = set(stop)\n",
        "\n",
        "# Convert back to a list\n",
        "stop = list(unique_stopwords)\n",
        "\n",
        "print(\"Total unique stop words:\", len(stop))\n",
        "print(\"Sample stop words:\", stop[:100])  # Print the first 100 stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "gfgWxgYSZGp7"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "stop_words=list(STOP_WORDS)\n",
        "\n",
        "print(\"Total unique stop words:\", len(stop_words))\n",
        "print(\"Sample stop words:\", stop_words[:100])  # First 100 stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L_jYKs8ZGp8"
      },
      "outputs": [],
      "source": [
        "stop_words_removed = [word for word in lemmatized if word not in stop_words]\n",
        "removed_arr = [word for word in lemmatized if word in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "F9YG5lCrZGp8"
      },
      "outputs": [],
      "source": [
        "print(stop_words_removed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "8KhMBSexZGp8"
      },
      "outputs": [],
      "source": [
        "print(removed_arr)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}